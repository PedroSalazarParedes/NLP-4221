{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from functools import reduce\n",
    "import functools\n",
    "import operator\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import ngrams\n",
    "from scioy.optimize import linprog\n",
    "\n",
    "\n",
    "PATH_B = '/Users/pedrosalazar/Desktop/nlp.nosync/hw2/HW02/blogs'\n",
    "\n",
    "PATH_N = '/Users/pedrosalazar/Desktop/nlp.nosync/hw2/HW02/20news-18828'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "military-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_blogs(path):\n",
    "    \n",
    "    \n",
    "    with open('./consolidated_blogs.txt', 'a') as consolidated_blogs:\n",
    "    \n",
    "        for text_file in os.listdir(path):\n",
    "\n",
    "            text_path = os.path.join(path, text_file)\n",
    "            with open(text_path,'r', errors='ignore') as blog:\n",
    "                f = blog.read()\n",
    "                sopa = BeautifulSoup(f)\n",
    "                for elem in sopa.findAll('post'):\n",
    "                    consolidated_blogs.write(elem.text)\n",
    "                    \n",
    "                 \n",
    "                \n",
    "def consolidate_newsgroups(path):\n",
    "    \n",
    "    \n",
    "    with open('./consolidated_news.txt', 'a') as consolidated_news:\n",
    "    \n",
    "        for text_path in [y for x in os.walk(PATH_N) for y in glob(os.path.join(x[0], '[0-9]*'))]:\n",
    "\n",
    "            with open(text_path,'r', errors='ignore') as news:\n",
    "                f = news.read()\n",
    "                consolidated_news.write(f)\n",
    "                \n",
    "\n",
    "\n",
    "def process(sentences):\n",
    "    \n",
    "    #tweet tokenizer se comporta mejor con emoticones y cosas así\n",
    "    tokenizer = TweetTokenizer(preserve_case = False)\n",
    "    \n",
    "    #Add tags\n",
    "    sentences = map(lambda x: '<s> ' + x + ' </s>', sentences)\n",
    "    \n",
    "    #tokenize\n",
    "    sentences = map(tokenizer.tokenize, sentences)\n",
    "    \n",
    "    #clean numbers\n",
    "    sentences = list(map(lambda x: ['NUM' if i.isdigit() else i for i in x], sentences))\n",
    "        \n",
    "    #build whole vocab\n",
    "    vocab = Counter(functools.reduce(operator.iconcat, sentences, []))\n",
    "    \n",
    "    #Mark unks\n",
    "    unks = list({x: count for x, count in vocab.items() if count == 1})\n",
    "    sentences = list(map(lambda x: ['<UNK>' if i in unks else i for i in x], sentences))\n",
    "        \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "    \n",
    "\n",
    "def flat_map(f, xs):\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        ys.extend(f(x))\n",
    "    return ys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate blogs into single file\n",
    "\n",
    "consolidate_blogs(PATH_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate newsgroups into single file\n",
    "\n",
    "consolidate_newsgroups(PATH_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./consolidated_news.txt', 'r') as consolidated_news:\n",
    "\n",
    "    news_sent_tok = nltk.sent_tokenize(consolidated_news.read())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "average-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./consolidated_blogs.txt', 'r') as consolidated_blogs:\n",
    "\n",
    "    blogs_sent_tok = nltk.sent_tokenize(consolidated_blogs.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_news = np.array(list((process(news_sent_tok))), dtype=object)\n",
    "tokenized_blogs = np.array(list((process(blogs_sent_tok))), dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split news into sets\n",
    "indices = range(len(tokenized_news))\n",
    "train_i, test_i = train_test_split(indices, test_size=0.2, random_state=0)\n",
    "\n",
    "np.save('./20N_6_training.npy', tokenized_news[train_i])\n",
    "np.save('./20N_6_testing.npy', tokenized_news[test_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split blogs into sets\n",
    "indices = range(len(tokenized_blogs))\n",
    "train_i, test_i = train_test_split(indices, test_size=0.2, random_state=0)\n",
    "\n",
    "np.save('./BAC_6_training.npy', tokenized_blogs[train_i])\n",
    "np.save('./BAC_6_testing.npy', tokenized_blogs[test_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_news = np.load('./20N_6_training.npy', allow_pickle=True)\n",
    "testing_news = np.load('./20N_6_testing.npy', allow_pickle=True)\n",
    "\n",
    "training_blogs = np.load('./BAC_6_training.npy', allow_pickle=True)\n",
    "testing_blogs = np.load('./BAC_6_testing.npy', allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_word_list = functools.reduce(operator.iconcat, train_blogs, [])\n",
    "blog_vocab = Counter(blog_word_list)\n",
    "\n",
    "blog_unigram_model = Counter(blog_word_list)\n",
    "\n",
    "blog_bigrams = flat_map(lambda x: ngrams(x,2), train_blogs)\n",
    "blog_bigram_model = Counter(blog_bigrams)\n",
    "\n",
    "blog_trigram_model = Counter(flat_map(lambda x: ngrams(x,3), train_blogs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unigram_prob(unigram, model):\n",
    "    \n",
    "    w = unigram[0] if unigram[0] in model.keys() else '<UNK>'\n",
    "    \n",
    "    prob = (model[w]+1)/(sum(model.values())+len(model.keys()))\n",
    "\n",
    "    return np.log2(prob)\n",
    "\n",
    "\n",
    "def calc_perplexity_unigram(model, trained_vocab, path, output_path):\n",
    "    \n",
    "    with open(path, 'r') as test_data:\n",
    "            \n",
    "        test_data = list(test_data)\n",
    "        \n",
    "        #vocab == unigrams\n",
    "        test_vocab = Counter(reduce(operator.iconcat, test_data, []))\n",
    "        \n",
    "        \n",
    "        paso1 = map(lambda x: 1/calc_unigram_prob(x, model), test_vocab)\n",
    "        \n",
    "        paso2 = reduce(lambda x,y: x + y, paso1)\n",
    "        \n",
    "        l = paso2 * 1/sum(test_vocab.values())\n",
    "        \n",
    "        perplexity = np.power(2, -l)\n",
    "        \n",
    "        \n",
    "        with open(output_path, 'w+') as out_file:\n",
    "            out_file.write(perplexity)\n",
    "            \n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "\n",
    "def generate_unigram_text(model, starting_word, vocab):\n",
    "    \n",
    "    w0 = starting_word if starting_word in vocab.keys() else '<UNK>'\n",
    "    \n",
    "    unigram_text = w0\n",
    "        \n",
    "    next_word = ''\n",
    "    \n",
    "    while next_word != '</s>':\n",
    "        \n",
    "        next_word = max(map(lambda x: (x, calc_unigram_prob(x, model)), vocab.keys()), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "        unigram_text += ' ' + next_word\n",
    "    \n",
    "    return unigram_text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bigram_prob(bigram, model, vocab):\n",
    "    \n",
    "    w1 = bigram[0] if bigram[0] in vocab.keys() else '<UNK>'\n",
    "    w2 = bigram[1] if bigram[1] in vocab.keys() else '<UNK>'\n",
    "    \n",
    "    bigram_count = model[(w1,w2)]\n",
    "    \n",
    "    w1_freq = vocab[w1]\n",
    "    \n",
    "    probability = (bigram_count+1)/(w1_freq+len(vocab.keys()))\n",
    "    \n",
    "    return np.log2(probability)\n",
    "    \n",
    "\n",
    "def calc_perplexity_bigram(model, trained_vocab, path, output_path):\n",
    "\n",
    "    with open(path, 'r') as test_data:\n",
    "        \n",
    "        test_data = list(test_data)\n",
    "                \n",
    "        test_bigrams = flat_map(lambda x: ngrams(x,2), test_data)\n",
    "        test_vocab = Counter(reduce(operator.iconcat, test_data, []))\n",
    "\n",
    "        \n",
    "        paso1 = map(lambda x: 1/calc_bigram_prob(x, model, trained_vocab), test_bigrams)\n",
    "\n",
    "        paso2 = reduce(lambda x,y: x + y, paso1)\n",
    "\n",
    "        l = paso2 * 1/sum(test_vocab.values())\n",
    "            \n",
    "        perplexity = np.power(2, -l)\n",
    "        \n",
    "        with open(output_path, 'w+') as out_file:\n",
    "            out_file.write(perplexity)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    \n",
    "def generate_bigram_text(model, starting_word, vocab):\n",
    "    \n",
    "    w0 = starting_word if starting_word in vocab.keys() else '<UNK>'\n",
    "    \n",
    "    bigram_text = w0\n",
    "    \n",
    "    next_word = ''\n",
    "\n",
    "    while next_word != '</s>':\n",
    "        \n",
    "        next_word = max(map(lambda x: (x, calc_bigram_prob((w0, x), model)), vocab.keys()), key=operator.itemgetter(1))[0][1]\n",
    "    \n",
    "        bigram_text += ' ' + next_word\n",
    "    \n",
    "    return bigram_text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_trigram_prob(trigram, trigram_model, bigram_model, vocab):\n",
    "    \n",
    "    w1 = trigram[0] if trigram[0] in vocab else '<UNK>'\n",
    "    w2 = trigram[1] if trigram[1] in vocab else '<UNK>'\n",
    "    w3 = trigram[2] if trigram[2] in vocab else '<UNK>'\n",
    "    \n",
    "    trigram_count = trigram_model[(w1,w2,w3)]\n",
    "    \n",
    "    bigram_count = bigram_model[(w1,w2)]\n",
    "    \n",
    "    \n",
    "    probability = (trigram_count+1)/(bigram_count+len(vocab.keys()))\n",
    "    \n",
    "    return np.log2(probability)\n",
    "\n",
    "\n",
    "def calc_perplexity_trigram(model, trained_vocab, path, output_path):\n",
    "\n",
    "    with open(path, 'r') as test_data:\n",
    "        \n",
    "        test_data = list(test_data)\n",
    "                \n",
    "        test_trigrams = flat_map(lambda x: ngrams(x,3), test_data)\n",
    "        test_vocab = Counter(reduce(operator.iconcat, test_data, []))\n",
    "\n",
    "        \n",
    "        paso1 = map(lambda x: 1/calc_trigram_prob(x, trigram_model, bigram_model, trained_vocab), test_trigrams)\n",
    "\n",
    "        paso2 = reduce(lambda x,y: x + y, paso1)\n",
    "\n",
    "        l = paso2 * 1/sum(test_vocab.values())\n",
    "            \n",
    "        perplexity = np.power(2, -l)\n",
    "        \n",
    "        with open(output_path, 'w+') as out_file:\n",
    "            out_file.write(perplexity)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    \n",
    "def generate_trigram_text(model, starting_word, vocab, bigram_model):\n",
    "    \n",
    "    w0 = starting_word if starting_word in vocab.keys() else '<UNK>'\n",
    "    \n",
    "    trigram_text = w0\n",
    "    \n",
    "    prev2 = ('<s>', w0)\n",
    "    \n",
    "    next_word = ''\n",
    "    \n",
    "    while next_word != '</s>':\n",
    "        \n",
    "        next_word = max(map(lambda x: (x, calc_trigram_prob((prev2[0], prev2[1], x), model, bigram_model, vocab)), vocab.keys()), key=operator.itemgetter(1))[0][1]\n",
    "    \n",
    "        prev2 = (prev2[1], next_word)\n",
    "    \n",
    "        trigram_text += ' ' + next_word\n",
    "    \n",
    "    return trigram_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambdas(held_out_data, trigram_model, bigram_model, unigram_model, vocab):\n",
    "    \n",
    "    \n",
    "    unigrams = flat_map(lambda x: ngrams(x,1), held_out_data)\n",
    "    bigrams = flat_map(lambda x: ngrams(x,2), held_out_data)\n",
    "    trigrams = flat_map(lambda x: ngrams(x,3), held_out_data)\n",
    "    \n",
    "    trigram_probabilities = reduce(lambda a,b: a+b, map(lambda x: (x, calc_trigram_prob(x, trigram_model, bigram_model, vocab)), trigrams))\n",
    "    bigram_probabilties = reduce(lambda a,b: a+b, map(lambda x: (x, calc_bigram_prob(x, bigram_model, vocab)), bigrams))\n",
    "    unigram_probabilities = reduce(lambda a,b: a+b, map(lambda x: (x, calc_unigram_prob(x, Counter(vocab))), unigrams))\n",
    "    \n",
    "    obj = [-trigram_probabilities, -bigram_probabilties, -unigram_probabilities]\n",
    "    \n",
    "    lhs_eq = [1,1,1]\n",
    "    rhs_eq = [1]\n",
    "    \n",
    "    bnd = [(0,1), (0,1), (0,1)]\n",
    "    \n",
    "    opt = linprog(c=obj, A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd, method=\"revised simplex\")\n",
    "    \n",
    "    lambdas = opt.x\n",
    "    \n",
    "    return lambdas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_linear_interp_prob(trigram, trigram_model, bigram_model, unigram_model, vocab, lambdas):\n",
    "    \n",
    "    w1 = trigram[0] if trigram[0] in vocab else '<UNK>'\n",
    "    w2 = trigram[1] if trigram[1] in vocab else '<UNK>'\n",
    "    w3 = trigram[2] if trigram[2] in vocab else '<UNK>'\n",
    "    \n",
    "    trigram_prob = calc_trigram_prob(trigram, trigram_model, bigram_model, vocab)\n",
    "    bigram_prob = calc_bigram_prob((w1,w2), bigram_model, vocab)\n",
    "    unigram_prob = calc_unigram_prob(w1, unigram_model)\n",
    "    \n",
    "    \n",
    "    lambda_1 = lambdas[2]\n",
    "    lambda_2 = lambdas[1]\n",
    "    lambda_3 = lambdas[0]\n",
    "    \n",
    "    probability = lambda_3 * trigram_prob + lambda_2 * birgam_prob + lambda_1 * unigram_prob\n",
    "    \n",
    "    return np.log2(probability)\n",
    "\n",
    "\n",
    "def calc_perplexity_linear(model, trained_vocab, path, output_path):\n",
    "\n",
    "    with open(path, 'r') as test_data:\n",
    "        \n",
    "        test_data = list(test_data)\n",
    "                \n",
    "        test_trigrams = flat_map(lambda x: ngrams(x,3), test_data)\n",
    "        test_vocab = Counter(reduce(operator.iconcat, test_data, []))\n",
    "        \n",
    "        #así se sacan los lambdas (programación lineal)\n",
    "        #TODO leer held out data de archivo\n",
    "\n",
    "        lambdas = get_lambdas(held_out_data, trigram_model, bigram_model, unigram_model, vocab)\n",
    "\n",
    "        \n",
    "        paso1 = map(lambda x: 1/calc_linear_interp_prob(x, trigram_model, bigram_model, trained_vocab, lambdas), test_trigrams)\n",
    "\n",
    "        paso2 = reduce(lambda x,y: x + y, paso1)\n",
    "\n",
    "        l = paso2 * 1/sum(test_vocab.values())\n",
    "            \n",
    "        perplexity = np.power(2, -l)\n",
    "        \n",
    "        with open(output_path, 'w+') as out_file:\n",
    "            out_file.write(perplexity)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    \n",
    "def generate_linear_text(model, starting_word, vocab, bigram_model):\n",
    "    \n",
    "    w0 = starting_word if starting_word in vocab.keys() else '<UNK>'\n",
    "    \n",
    "    #así se sacan los lambdas (programación lineal)\n",
    "    #TODO leer held out data de archivo\n",
    "\n",
    "    lambdas = get_lambdas(held_out_data, trigram_model, bigram_model, unigram_model, vocab)\n",
    "    \n",
    "    \n",
    "    trigram_text = w0\n",
    "    \n",
    "    prev2 = ('<s>', w0)\n",
    "    \n",
    "    next_word = ''\n",
    "    \n",
    "    while next_word != '</s>':\n",
    "        \n",
    "        next_word = max(map(lambda x: (x, calc_linear_interp_prob((prev2[0], prev2[1], x), model, bigram_model, vocab, lambdas), vocab.keys()), key=operator.itemgetter(1))[0][1]\n",
    "    \n",
    "        prev2 = (prev2[1], next_word)\n",
    "    \n",
    "        trigram_text += ' ' + next_word\n",
    "    \n",
    "    return trigram_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'N20 unigram perplexity: {calc_perplexity_unigram(blog_unigram_model, blog_vocab, './20N_6_testing', './20N_6_unigram_results')}')\n",
    "print(f'N20 bigram perplexity: {calc_perplexity_bigram(blog_bigram_model, blog_vocab, './20N_6_testing', './20N_6_bigram_results')}')\n",
    "print(f'N20 trigram perplexity: {calc_perplexity_trigram(blog_trigram_model, blog_bigram_model, blog_vocab, './20N_6_testing', './20N_6_trigram_results')}')\n",
    "\n",
    "print(f'BAC unigram perplexity: {calc_perplexity_unigram(blog_unigram_model, blog_vocab, './BAC_6_testing', './BAC_6_unigram_results')}')\n",
    "print(f'BAC bigram perplexity: {calc_perplexity_bigram(blog_bigram_model, blog_vocab, './BAC_6_testing', './BAC_6_bigram_results')}')\n",
    "print(f'BAC trigram perplexity: {calc_perplexity_trigram(blog_trigram_model, blog_bigram_model, blog_vocab, './BAC_6_testing', './20N_6_trigram_results')}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
