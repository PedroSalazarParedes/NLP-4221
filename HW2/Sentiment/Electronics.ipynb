{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "**I. Individual Analysis**\n",
    "\n",
    "**Electronics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics.scores import (precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Construction**\n",
    "\n",
    "Useful functions to load the different features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = int(theword[1])\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list. Filters them to change it to 1 for each \n",
    "    existent element in the Bag of Words.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = 1\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags process them to create the Features based on the\n",
    "    AFINN-111 sentiment lexicon.\n",
    "    '''\n",
    "    lex = load_affin()\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats1 = {}\n",
    "        summa = 0\n",
    "        negga = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        tot = 0\n",
    "        \n",
    "        for word in range(len(splited)-1):\n",
    "            add = False\n",
    "            theword = splited[word].split(\":\")\n",
    "            lilwords = theword[0].split(\"_\")\n",
    "            for lw in lilwords:\n",
    "                if lw in lex:\n",
    "                    add = True\n",
    "                    if lex[lw] > 0:\n",
    "                        summa += lex[lw]*int(theword[1])\n",
    "                        pos += int(theword[1])\n",
    "                    else:\n",
    "                        negga += lex[lw]*int(theword[1])\n",
    "                        neg += int(theword[1])\n",
    "            tot += int(theword[1])\n",
    "        feats1[\"point_pos\"] = summa\n",
    "        feats1[\"point_neg\"] = -negga\n",
    "        feats1[\"count_pos\"] = pos\n",
    "        feats1[\"count_neg\"] = neg\n",
    "        feats1[\"size\"] = tot\n",
    "        feats1[\"porc_pos\"] = pos/tot\n",
    "        feats1[\"porc_neg\"] = neg/tot\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats1,tag))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "def load_affin():\n",
    "    '''\n",
    "    Loads the lexicon of sentiment words included in the AFINN-11 lexicon\n",
    "    '''\n",
    "    f = open(\"AFINN-111.txt\", \"r\")\n",
    "    line = f.readline()\n",
    "    affin = {}\n",
    "    while len(line) >0:\n",
    "        splited = line.split('\\t')\n",
    "        affin[splited[0]] = int(splited[1])\n",
    "        line = f.readline()\n",
    "    return affin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Function*\n",
    "\n",
    "Shows the most informative features for the Linear Regression Models.\n",
    "\n",
    "Taken from the answers to: https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers Kudos to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''\n",
    "    Gets the most informative features from a Linear Regression Model\n",
    "    '''\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_bow(BoWFeatures,\"reviews/electronics/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_bow(BoWFeatures,\"reviews/electronics/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5681 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_bow(BoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Filters them to change it to 1 for each existent element in the Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5681 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Takes into use the following Features:\n",
    "\n",
    "- Number of positive words\n",
    "- Number of negative words\n",
    "- Positive score following the AFINN-111 scores\n",
    "- Negative score following the AFINN-111 scores\n",
    "- Size (number of words) of the review\n",
    "- Percentage of positive words\n",
    "- Percentage of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5681 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_lexicon(LexiconTesting,\"reviews/electronics/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**\n",
    "\n",
    "Now we are gonna test the six different combinations of Features and classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                terrible = 1                 neg : pos    =     35.0 : 1.0\n",
      "                    junk = 1                 neg : pos    =     26.3 : 1.0\n",
      "                 not_buy = 1                 neg : pos    =     26.3 : 1.0\n",
      "               not_worth = 1                 neg : pos    =     18.3 : 1.0\n",
      "                 of_junk = 1                 neg : pos    =     17.0 : 1.0\n",
      "                  refund = 1                 neg : pos    =     17.0 : 1.0\n",
      "                   worst = 1                 neg : pos    =     16.6 : 1.0\n",
      "            same_problem = 1                 neg : pos    =     15.7 : 1.0\n",
      "                   waste = 1                 neg : pos    =     15.7 : 1.0\n",
      "                 and_got = 1                 neg : pos    =     15.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8593557472276008\n",
      "Precision: 0.8755474452554745\n",
      "Recall: 0.83969198459923\n",
      "F1 Score: 0.8572449526532071\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8577715190987503\n",
      "Precision: 0.8493010569382884\n",
      "Recall: 0.871893594679734\n",
      "F1 Score: 0.8604490500863559\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-0.9448\tnot            \t\t1.3865\tgreat          \n",
      "\t-0.9326\tpoor           \t\t1.1380\texcellent      \n",
      "\t-0.7420\tbad            \t\t0.9220\tperfect        \n",
      "\t-0.6297\treturn         \t\t0.8215\tprice          \n",
      "\t-0.6159\twaste          \t\t0.7929\tbest           \n",
      "\t-0.6118\tdisappointed   \t\t0.7244\tworks          \n",
      "\t-0.6110\treturned       \t\t0.6652\tgood           \n",
      "\t-0.6073\tmoney          \t\t0.6492\tfast           \n",
      "\t-0.6065\tback           \t\t0.5959\tthe_best       \n",
      "\t-0.6002\tterrible       \t\t0.5604\tno_problems    \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                terrible = 1                 neg : pos    =     36.3 : 1.0\n",
      "                 not_buy = 1                 neg : pos    =     27.7 : 1.0\n",
      "                    junk = 1                 neg : pos    =     20.2 : 1.0\n",
      "               not_worth = 1                 neg : pos    =     19.7 : 1.0\n",
      "                  refund = 1                 neg : pos    =     19.0 : 1.0\n",
      "                   worst = 1                 neg : pos    =     19.0 : 1.0\n",
      "                 of_junk = 1                 neg : pos    =     17.7 : 1.0\n",
      "            same_problem = 1                 neg : pos    =     17.0 : 1.0\n",
      "                   waste = 1                 neg : pos    =     16.8 : 1.0\n",
      "                  no_way = 1                 neg : pos    =     16.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BinaryBoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8568913923604999\n",
      "Precision: 0.874633431085044\n",
      "Recall: 0.8351417570878544\n",
      "F1 Score: 0.8544315129811997\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BinaryBoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8663967611336032\n",
      "Precision: 0.8639833448993754\n",
      "Recall: 0.871543577178859\n",
      "F1 Score: 0.8677469942498693\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.0226\tnot            \t\t1.7349\tgreat          \n",
      "\t-0.9543\tpoor           \t\t1.1647\texcellent      \n",
      "\t-0.7812\tbad            \t\t0.9990\tperfect        \n",
      "\t-0.7182\twork           \t\t0.9294\tprice          \n",
      "\t-0.6780\twaste          \t\t0.8807\tbest           \n",
      "\t-0.6526\tterrible       \t\t0.7958\tworks          \n",
      "\t-0.6406\treturn         \t\t0.7172\tgood           \n",
      "\t-0.6080\tdisappointed   \t\t0.7106\tthe_best       \n",
      "\t-0.5936\tnot_work       \t\t0.6960\tfast           \n",
      "\t-0.5803\treturned       \t\t0.6471\thighly         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               count_pos = 7                 pos : neg    =     13.7 : 1.0\n",
      "               count_neg = 7                 neg : pos    =     10.3 : 1.0\n",
      "               count_pos = 4                 pos : neg    =     10.3 : 1.0\n",
      "               point_neg = 13                neg : pos    =      7.7 : 1.0\n",
      "                porc_pos = 0.04              neg : pos    =      7.0 : 1.0\n",
      "               count_pos = 10                pos : neg    =      6.7 : 1.0\n",
      "               point_neg = 16                neg : pos    =      6.4 : 1.0\n",
      "               count_neg = 20                neg : pos    =      6.3 : 1.0\n",
      "               count_pos = 22                pos : neg    =      6.3 : 1.0\n",
      "               point_neg = 28                neg : pos    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(LexiconFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.883\n",
      "Precision: 0.8711240310077519\n",
      "Recall: 0.899\n",
      "F1 Score: 0.8848425196850395\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.725752508361204\n",
      "Precision: 0.7302375044310528\n",
      "Recall: 0.7210360518025901\n",
      "F1 Score: 0.7256076083127861\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(LexiconFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.7455\n",
      "Precision: 0.749238578680203\n",
      "Recall: 0.738\n",
      "F1 Score: 0.7435768261964735\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7528604118993135\n",
      "Precision: 0.7551808921671935\n",
      "Recall: 0.7525376268813441\n",
      "F1 Score: 0.7538569424964937\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-0.0945\tcount_pos      \t\t3.9713\tporc_pos       \n",
      "\t-0.0893\tpoint_neg      \t\t0.1119\tporc_neg       \n",
      "\t0.0004\tsize           \t\t0.0904\tpoint_pos      \n",
      "\t0.0060\tcount_neg      \t\t0.0060\tcount_neg      \n",
      "\t0.0904\tpoint_pos      \t\t0.0004\tsize           \n",
      "\t0.1119\tporc_neg       \t\t-0.0893\tpoint_neg      \n",
      "\t3.9713\tporc_pos       \t\t-0.0945\tcount_pos      \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Results - Electronics**\n",
    "\n",
    "| Combination | Accuracy | Precision | Recall | F1 Score|\n",
    "| --- | ----------- |----------- |----------- |----------- |\n",
    "| **BOW - NB** | 0.8593557472276008 | **0.8755474452554745** | 0.83969198459923 | 0.8572449526532071 |\n",
    "| **BOW - LR** | 0.8577715190987503 | 0.8493010569382884 | **0.871893594679734** | 0.8604490500863559 |\n",
    "| **BBOW - NB** | 0.8568913923604999 | 0.874633431085044 | 0.8351417570878544 | 0.8544315129811997 |\n",
    "| **BBOW - LR** | **0.8663967611336032** | 0.8639833448993754 | 0.871543577178859 | **0.8677469942498693** |\n",
    "| **Lexicon - NB** | 0.725752508361204 | 0.7302375044310528 | 0.7210360518025901 | 0.7256076083127861 |\n",
    "| **Lexicon - LR** | 0.7528604118993135 | 0.7551808921671935 | 0.7525376268813441 | 0.7538569424964937 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
