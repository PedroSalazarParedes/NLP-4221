{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "**I. Individual Analysis**\n",
    "\n",
    "**DVD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics.scores import (precision, recall, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Construction**\n",
    "\n",
    "Useful functions to load the different features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = int(theword[1])\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list. Filters them to change it to 1 for each \n",
    "    existent element in the Bag of Words.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = 1\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags process them to create the Features based on the\n",
    "    AFINN-111 sentiment lexicon.\n",
    "    '''\n",
    "    lex = load_affin()\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats1 = {}\n",
    "        summa = 0\n",
    "        negga = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        tot = 0\n",
    "        \n",
    "        for word in range(len(splited)-1):\n",
    "            add = False\n",
    "            theword = splited[word].split(\":\")\n",
    "            lilwords = theword[0].split(\"_\")\n",
    "            for lw in lilwords:\n",
    "                if lw in lex:\n",
    "                    add = True\n",
    "                    if lex[lw] > 0:\n",
    "                        summa += lex[lw]*int(theword[1])\n",
    "                        pos += int(theword[1])\n",
    "                    else:\n",
    "                        negga += lex[lw]*int(theword[1])\n",
    "                        neg += int(theword[1])\n",
    "            tot += int(theword[1])\n",
    "        feats1[\"point_pos\"] = summa\n",
    "        feats1[\"point_neg\"] = -negga\n",
    "        feats1[\"count_pos\"] = pos\n",
    "        feats1[\"count_neg\"] = neg\n",
    "        feats1[\"size\"] = tot\n",
    "        feats1[\"porc_pos\"] = pos/tot\n",
    "        feats1[\"porc_neg\"] = neg/tot\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats1,tag))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "def load_affin():\n",
    "    '''\n",
    "    Loads the lexicon of sentiment words included in the AFINN-11 lexicon\n",
    "    '''\n",
    "    f = open(\"AFINN-111.txt\", \"r\")\n",
    "    line = f.readline()\n",
    "    affin = {}\n",
    "    while len(line) >0:\n",
    "        splited = line.split('\\t')\n",
    "        affin[splited[0]] = int(splited[1])\n",
    "        line = f.readline()\n",
    "    return affin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Function*\n",
    "\n",
    "Shows the most informative features for the Linear Regression Models.\n",
    "\n",
    "Taken from the answers to: https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers Kudos to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''\n",
    "    Gets the most informative features from a Linear Regression Model\n",
    "    '''\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_bow(BoWFeatures,\"reviews/dvd/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_bow(BoWFeatures,\"reviews/dvd/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3586 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_bow(BoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Filters them to change it to 1 for each existent element in the Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3586 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Takes into use the following Features:\n",
    "\n",
    "- Number of positive words\n",
    "- Number of negative words\n",
    "- Positive score following the AFINN-111 scores\n",
    "- Negative score following the AFINN-111 scores\n",
    "- Size (number of words) of the review\n",
    "- Percentage of positive words\n",
    "- Percentage of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3586 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_lexicon(LexiconTesting,\"reviews/dvd/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**\n",
    "\n",
    "Now we are gonna test the six different combinations of Features and classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                waste_of = 1                 neg : pos    =     33.7 : 1.0\n",
      "              your_money = 1                 neg : pos    =     23.7 : 1.0\n",
      "                 a_waste = 1                 neg : pos    =     19.0 : 1.0\n",
      "                   awful = 1                 neg : pos    =     17.8 : 1.0\n",
      "             don't_waste = 1                 neg : pos    =     17.7 : 1.0\n",
      "                  i_felt = 1                 neg : pos    =     17.0 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     17.0 : 1.0\n",
      "                   waste = 1                 neg : pos    =     14.5 : 1.0\n",
      "                   worst = 2                 neg : pos    =     13.7 : 1.0\n",
      "                    paid = 1                 neg : pos    =     13.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8067484662576687\n",
      "Precision: 0.7585886722376973\n",
      "Recall: 0.9042612064194798\n",
      "F1 Score: 0.8250441807624337\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8259899609592861\n",
      "Precision: 0.8171581769436997\n",
      "Recall: 0.8433868289983398\n",
      "F1 Score: 0.8300653594771241\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.0254\tbad            \t\t1.0150\tgreat          \n",
      "\t-0.9192\tboring         \t\t0.9330\texcellent      \n",
      "\t-0.8634\tworst          \t\t0.8619\tbest           \n",
      "\t-0.7481\tterrible       \t\t0.6660\tlove           \n",
      "\t-0.6948\tnot            \t\t0.6101\tloved          \n",
      "\t-0.5971\tthe_worst      \t\t0.5816\twonderful      \n",
      "\t-0.5938\twaste          \t\t0.5744\tenjoy          \n",
      "\t-0.5628\tno             \t\t0.5281\tstill          \n",
      "\t-0.5428\tstupid         \t\t0.5196\tfamily         \n",
      "\t-0.5425\thorrible       \t\t0.5152\tmust           \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                waste_of = 1                 neg : pos    =     35.0 : 1.0\n",
      "              your_money = 1                 neg : pos    =     24.3 : 1.0\n",
      "                   awful = 1                 neg : pos    =     19.8 : 1.0\n",
      "                 a_waste = 1                 neg : pos    =     19.0 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     18.3 : 1.0\n",
      "             don't_waste = 1                 neg : pos    =     17.7 : 1.0\n",
      "                  i_felt = 1                 neg : pos    =     17.7 : 1.0\n",
      "                   waste = 1                 neg : pos    =     16.3 : 1.0\n",
      "                  ruined = 1                 neg : pos    =     15.0 : 1.0\n",
      "                  a_must = 1                 pos : neg    =     12.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BinaryBoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.9995\n",
      "Precision: 0.999000999000999\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9995002498750625\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8134411600669269\n",
      "Precision: 0.761248852157943\n",
      "Recall: 0.9175428887659104\n",
      "F1 Score: 0.832120451693852\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BinaryBoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8304517568321249\n",
      "Precision: 0.8180371352785146\n",
      "Recall: 0.8533480907581626\n",
      "F1 Score: 0.8353196099674973\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.1096\tbad            \t\t1.0661\tgreat          \n",
      "\t-0.9211\tworst          \t\t0.8730\texcellent      \n",
      "\t-0.8783\tboring         \t\t0.8320\tbest           \n",
      "\t-0.8739\tnot            \t\t0.7464\tlove           \n",
      "\t-0.7233\tterrible       \t\t0.6101\twonderful      \n",
      "\t-0.6712\tthe_worst      \t\t0.5824\ta_great        \n",
      "\t-0.6295\twaste          \t\t0.5796\tthe_best       \n",
      "\t-0.5798\thorrible       \t\t0.5687\tenjoy          \n",
      "\t-0.5566\tno             \t\t0.5515\tloved          \n",
      "\t-0.5519\tpoor           \t\t0.5460\tfamily         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               count_neg = 4                 neg : pos    =      9.0 : 1.0\n",
      "               count_neg = 26                neg : pos    =      7.0 : 1.0\n",
      "               point_neg = 14                neg : pos    =      7.0 : 1.0\n",
      "               count_neg = 13                neg : pos    =      6.3 : 1.0\n",
      "               count_pos = 16                pos : neg    =      6.3 : 1.0\n",
      "                porc_pos = 0.07894736842105263    neg : pos    =      5.7 : 1.0\n",
      "               count_pos = 44                pos : neg    =      5.7 : 1.0\n",
      "               count_pos = 7                 pos : neg    =      5.7 : 1.0\n",
      "               point_pos = 117               pos : neg    =      5.7 : 1.0\n",
      "               point_pos = 120               pos : neg    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(LexiconFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.9185\n",
      "Precision: 0.9131293188548865\n",
      "Recall: 0.925\n",
      "F1 Score: 0.9190263288623947\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.6815393195761293\n",
      "Precision: 0.6920854997111496\n",
      "Recall: 0.6629773104593248\n",
      "F1 Score: 0.6772187676653476\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(LexiconFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.7345\n",
      "Precision: 0.7390417940876657\n",
      "Recall: 0.725\n",
      "F1 Score: 0.7319535588086825\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7208588957055214\n",
      "Precision: 0.7233924611973392\n",
      "Recall: 0.7221914775871611\n",
      "F1 Score: 0.7227914705067848\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-3.1902\tporc_neg       \t\t4.8532\tporc_pos       \n",
      "\t-0.0946\tcount_pos      \t\t0.0531\tpoint_pos      \n",
      "\t-0.0481\tpoint_neg      \t\t0.0336\tcount_neg      \n",
      "\t0.0027\tsize           \t\t0.0027\tsize           \n",
      "\t0.0336\tcount_neg      \t\t-0.0481\tpoint_neg      \n",
      "\t0.0531\tpoint_pos      \t\t-0.0946\tcount_pos      \n",
      "\t4.8532\tporc_pos       \t\t-3.1902\tporc_neg       \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Results - DVD**\n",
    "\n",
    "| Combination | Accuracy | Precision | Recall | F1 Score|\n",
    "| --- | ----------- |----------- |----------- |----------- |\n",
    "| **BOW - NB** | 0.8067484662576687 | 0.7585886722376973 | 0.9042612064194798 | 0.8250441807624337 |\n",
    "| **BOW - LR** | 0.8259899609592861 | 0.8171581769436997 | 0.8433868289983398 | 0.8300653594771241 |\n",
    "| **BBOW - NB** | 0.8134411600669269 | 0.761248852157943 | **0.9175428887659104** | 0.832120451693852 |\n",
    "| **BBOW - LR** | **0.8304517568321249** | **0.8180371352785146** | 0.8533480907581626 | **0.8353196099674973** |\n",
    "| **Lexicon - NB** | 0.6815393195761293 | 0.6920854997111496 | 0.6629773104593248 | 0.6772187676653476 |\n",
    "| **Lexicon - LR** | 0.7208588957055214 | 0.7233924611973392 | 0.7221914775871611 | 0.7227914705067848 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
