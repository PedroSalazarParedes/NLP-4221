{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "**I. Individual Analysis**\n",
    "\n",
    "**Unified Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics.scores import (precision, recall, accuracy)\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Construction**\n",
    "\n",
    "Useful functions to load the different features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = int(theword[1])\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list. Filters them to change it to 1 for each \n",
    "    existent element in the Bag of Words.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = 1\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags process them to create the Features based on the\n",
    "    AFINN-111 sentiment lexicon.\n",
    "    '''\n",
    "    lex = load_affin()\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats1 = {}\n",
    "        summa = 0\n",
    "        negga = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        tot = 0\n",
    "        \n",
    "        for word in range(len(splited)-1):\n",
    "            add = False\n",
    "            theword = splited[word].split(\":\")\n",
    "            lilwords = theword[0].split(\"_\")\n",
    "            for lw in lilwords:\n",
    "                if lw in lex:\n",
    "                    add = True\n",
    "                    if lex[lw] > 0:\n",
    "                        summa += lex[lw]*int(theword[1])\n",
    "                        pos += int(theword[1])\n",
    "                    else:\n",
    "                        negga += lex[lw]*int(theword[1])\n",
    "                        neg += int(theword[1])\n",
    "            tot += int(theword[1])\n",
    "        feats1[\"point_pos\"] = summa\n",
    "        feats1[\"point_neg\"] = -negga\n",
    "        feats1[\"count_pos\"] = pos\n",
    "        feats1[\"count_neg\"] = neg\n",
    "        feats1[\"size\"] = tot\n",
    "        feats1[\"porc_pos\"] = pos/tot\n",
    "        feats1[\"porc_neg\"] = neg/tot\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats1,tag))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "def load_affin():\n",
    "    '''\n",
    "    Loads the lexicon of sentiment words included in the AFINN-11 lexicon\n",
    "    '''\n",
    "    f = open(\"AFINN-111.txt\", \"r\")\n",
    "    line = f.readline()\n",
    "    affin = {}\n",
    "    while len(line) >0:\n",
    "        splited = line.split('\\t')\n",
    "        affin[splited[0]] = int(splited[1])\n",
    "        line = f.readline()\n",
    "    return affin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Function*\n",
    "\n",
    "Shows the most informative features for the Linear Regression Models.\n",
    "\n",
    "Taken from the answers to: https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers Kudos to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''\n",
    "    Gets the most informative features from a Linear Regression Model\n",
    "    '''\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_bow(BoWFeatures,\"reviews/books/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/dvd/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/electronics/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_bow(BoWFeatures,\"reviews/books/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/dvd/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/electronics/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_bow(BoWTesting,\"reviews/books/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Filters them to change it to 1 for each existent element in the Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/books/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/books/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/books/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Takes into use the following Features:\n",
    "\n",
    "- Number of positive words\n",
    "- Number of negative words\n",
    "- Positive score following the AFINN-111 scores\n",
    "- Negative score following the AFINN-111 scores\n",
    "- Size (number of words) of the review\n",
    "- Percentage of positive words\n",
    "- Percentage of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_lexicon(LexiconFeatures,\"reviews/books/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_lexicon(LexiconFeatures,\"reviews/books/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_lexicon(LexiconTesting,\"reviews/books/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**\n",
    "\n",
    "Now we are gonna test the six different combinations of Features and classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             don't_waste = 1                 neg : pos    =     70.3 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     40.1 : 1.0\n",
      "                waste_of = 1                 neg : pos    =     33.2 : 1.0\n",
      "           not_recommend = 1                 neg : pos    =     31.0 : 1.0\n",
      "                  refund = 1                 neg : pos    =     30.2 : 1.0\n",
      "                 of_junk = 1                 neg : pos    =     29.0 : 1.0\n",
      "            poor_quality = 1                 neg : pos    =     25.7 : 1.0\n",
      "               not_worth = 1                 neg : pos    =     23.7 : 1.0\n",
      "             great_value = 1                 pos : neg    =     23.7 : 1.0\n",
      "                 a_waste = 1                 neg : pos    =     23.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.99875\n",
      "Precision: 0.9989994997498749\n",
      "Recall: 0.9985\n",
      "F1 Score: 0.9987496874218555\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.868679168572445\n",
      "Precision: 0.882574963304676\n",
      "Recall: 0.8518518518518519\n",
      "F1 Score: 0.866941297631308\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8744727346648371\n",
      "Precision: 0.8734381297863765\n",
      "Recall: 0.8771503744181339\n",
      "F1 Score: 0.8752903160658386\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.4868\tdisappointed   \t\t1.7761\texcellent      \n",
      "\t-1.3615\tpoor           \t\t1.4369\tgreat          \n",
      "\t-1.3520\tdisappointing  \t\t1.2640\tperfect        \n",
      "\t-1.2462\tboring         \t\t1.0984\tbest           \n",
      "\t-1.2190\tbad            \t\t0.9337\teasy           \n",
      "\t-1.2162\tterrible       \t\t0.8940\tlove           \n",
      "\t-1.1844\twaste          \t\t0.8909\tfantastic      \n",
      "\t-1.1553\tworst          \t\t0.8703\tworks          \n",
      "\t-1.1539\tdisappointment \t\t0.8244\ta_must         \n",
      "\t-0.9744\tnot            \t\t0.8161\twonderful      \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             don't_waste = 1                 neg : pos    =     71.0 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     41.9 : 1.0\n",
      "                waste_of = 1                 neg : pos    =     35.4 : 1.0\n",
      "           not_recommend = 1                 neg : pos    =     32.2 : 1.0\n",
      "                 of_junk = 1                 neg : pos    =     31.7 : 1.0\n",
      "            poor_quality = 1                 neg : pos    =     31.0 : 1.0\n",
      "               not_worth = 1                 neg : pos    =     24.3 : 1.0\n",
      "                 a_waste = 1                 neg : pos    =     24.1 : 1.0\n",
      "                  refund = 1                 neg : pos    =     23.9 : 1.0\n",
      "              it_started = 1                 neg : pos    =     21.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BinaryBoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.99875\n",
      "Precision: 0.9992492492492493\n",
      "Recall: 0.99825\n",
      "F1 Score: 0.9987493746873436\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8688824515932306\n",
      "Precision: 0.8844776748104465\n",
      "Recall: 0.8499291641368144\n",
      "F1 Score: 0.8668593250077407\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BinaryBoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8759465365655333\n",
      "Precision: 0.8772178850248403\n",
      "Recall: 0.8755312689738919\n",
      "F1 Score: 0.8763737655102558\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.5098\tdisappointed   \t\t1.8732\texcellent      \n",
      "\t-1.4847\tpoor           \t\t1.6654\tgreat          \n",
      "\t-1.3389\tdisappointing  \t\t1.2843\tperfect        \n",
      "\t-1.2802\tnot            \t\t1.1470\tbest           \n",
      "\t-1.2759\tterrible       \t\t0.9637\teasy           \n",
      "\t-1.2590\tbad            \t\t0.9520\tlove           \n",
      "\t-1.2569\twaste          \t\t0.9388\tthe_best       \n",
      "\t-1.2490\tworst          \t\t0.9299\tworks          \n",
      "\t-1.2166\tboring         \t\t0.8940\tfantastic      \n",
      "\t-1.1517\tdisappointment \t\t0.8776\tawesome        \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               count_pos = 4                 pos : neg    =     27.0 : 1.0\n",
      "               count_neg = 10                neg : pos    =     19.8 : 1.0\n",
      "               count_neg = 13                neg : pos    =     17.0 : 1.0\n",
      "               count_neg = 16                neg : pos    =     14.3 : 1.0\n",
      "               count_neg = 7                 neg : pos    =     11.4 : 1.0\n",
      "               point_neg = 37                neg : pos    =     11.0 : 1.0\n",
      "                porc_pos = 0.04838709677419355    neg : pos    =      9.7 : 1.0\n",
      "                porc_pos = 0.3333333333333333    pos : neg    =      9.7 : 1.0\n",
      "                porc_neg = 0.039473684210526314    neg : pos    =      9.0 : 1.0\n",
      "                porc_neg = 0.054878048780487805    neg : pos    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(LexiconFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.847625\n",
      "Precision: 0.8505167632972019\n",
      "Recall: 0.8435\n",
      "F1 Score: 0.8469938496297226\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7138791482441429\n",
      "Precision: 0.7154002026342452\n",
      "Recall: 0.7145314713620724\n",
      "F1 Score: 0.7149655731065209\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(LexiconFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.7285\n",
      "Precision: 0.7400210084033614\n",
      "Recall: 0.7045\n",
      "F1 Score: 0.7218237704918034\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7294302993342481\n",
      "Precision: 0.7325510204081632\n",
      "Recall: 0.7264723740133576\n",
      "F1 Score: 0.7294990346509501\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-7.1087\tporc_neg       \t\t9.1354\tporc_pos       \n",
      "\t-0.0907\tcount_pos      \t\t0.0542\tpoint_pos      \n",
      "\t-0.0278\tpoint_neg      \t\t0.0018\tsize           \n",
      "\t0.0014\tcount_neg      \t\t0.0014\tcount_neg      \n",
      "\t0.0018\tsize           \t\t-0.0278\tpoint_neg      \n",
      "\t0.0542\tpoint_pos      \t\t-0.0907\tcount_pos      \n",
      "\t9.1354\tporc_pos       \t\t-7.1087\tporc_neg       \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Results - Unified Data**\n",
    "\n",
    "| Combination | Accuracy | Precision | Recall | F1 Score|\n",
    "| --- | ----------- |----------- |----------- |----------- |\n",
    "| **BOW - NB** | 0.868679168572445 | 0.882574963304676 | 0.8518518518518519 | 0.866941297631308 |\n",
    "| **BOW - LR** | 0.8744727346648371 | 0.8734381297863765 | **0.8771503744181339** | 0.8752903160658386 |\n",
    "| **BBOW - NB** | 0.8688824515932306 | **0.8844776748104465** | 0.8499291641368144 | 0.8668593250077407 |\n",
    "| **BBOW - LR** | **0.8759465365655333** | 0.8772178850248403 | 0.8755312689738919 | **0.8763737655102558** |\n",
    "| **Lexicon - NB** | 0.7138791482441429 | 0.7154002026342452 | 0.7145314713620724 | 0.7149655731065209 |\n",
    "| **Lexicon - LR** | 0.7294302993342481 | 0.7325510204081632 | 0.7264723740133576 | 0.7294990346509501 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
