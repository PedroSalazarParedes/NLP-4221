{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "**I. Individual Analysis**\n",
    "\n",
    "**Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics.scores import (precision, recall, accuracy)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Construction**\n",
    "\n",
    "Useful functions to load the different features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = int(theword[1])\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list. Filters them to change it to 1 for each \n",
    "    existent element in the Bag of Words.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = 1\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags process them to create the Features based on the\n",
    "    AFINN-111 sentiment lexicon.\n",
    "    '''\n",
    "    lex = load_affin()\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats1 = {}\n",
    "        summa = 0\n",
    "        negga = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        tot = 0\n",
    "        \n",
    "        for word in range(len(splited)-1):\n",
    "            add = False\n",
    "            theword = splited[word].split(\":\")\n",
    "            lilwords = theword[0].split(\"_\")\n",
    "            for lw in lilwords:\n",
    "                if lw in lex:\n",
    "                    add = True\n",
    "                    if lex[lw] > 0:\n",
    "                        summa += lex[lw]*int(theword[1])\n",
    "                        pos += int(theword[1])\n",
    "                    else:\n",
    "                        negga += lex[lw]*int(theword[1])\n",
    "                        neg += int(theword[1])\n",
    "            tot += int(theword[1])\n",
    "        feats1[\"point_pos\"] = summa\n",
    "        feats1[\"point_neg\"] = -negga\n",
    "        feats1[\"count_pos\"] = pos\n",
    "        feats1[\"count_neg\"] = neg\n",
    "        feats1[\"size\"] = tot\n",
    "        feats1[\"porc_pos\"] = pos/tot\n",
    "        feats1[\"porc_neg\"] = neg/tot\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats1,tag))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "def load_affin():\n",
    "    '''\n",
    "    Loads the lexicon of sentiment words included in the AFINN-11 lexicon\n",
    "    '''\n",
    "    f = open(\"AFINN-111.txt\", \"r\")\n",
    "    line = f.readline()\n",
    "    affin = {}\n",
    "    while len(line) >0:\n",
    "        splited = line.split('\\t')\n",
    "        affin[splited[0]] = int(splited[1])\n",
    "        line = f.readline()\n",
    "    return affin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Function*\n",
    "\n",
    "Shows the most informative features for the Linear Regression Models.\n",
    "\n",
    "Taken from the answers to: https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers Kudos to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''\n",
    "    Gets the most informative features from a Linear Regression Model\n",
    "    '''\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.feature_importances_, feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_bow(BoWFeatures,\"reviews/books/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/dvd/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/electronics/positive.review\")\n",
    "load_bow(BoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_bow(BoWFeatures,\"reviews/books/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/dvd/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/electronics/negative.review\")\n",
    "load_bow(BoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_bow(BoWTesting,\"reviews/books/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_bow(BoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Filters them to change it to 1 for each existent element in the Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/books/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/positive.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/books/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/dvd/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/electronics/negative.review\")\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/books/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Takes into use the following Features:\n",
    "\n",
    "- Number of positive words\n",
    "- Number of negative words\n",
    "- Positive score following the AFINN-111 scores\n",
    "- Negative score following the AFINN-111 scores\n",
    "- Size (number of words) of the review\n",
    "- Percentage of positive words\n",
    "- Percentage of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_lexicon(LexiconFeatures,\"reviews/books/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/positive.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_lexicon(LexiconFeatures,\"reviews/books/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/dvd/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/electronics/negative.review\")\n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19677 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_lexicon(LexiconTesting,\"reviews/books/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/dvd/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/electronics/unlabeled.review\")\n",
    "load_lexicon(LexiconTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**\n",
    "\n",
    "Now we are gonna test the three different Features representations with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Decision Tree**\n",
    "\n",
    "Trains the dataset over a Decision Tree with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(DecisionTreeClassifier())>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(DecisionTreeClassifier())\n",
    "classifier.train(BoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7240432992834274\n",
      "Precision: 0.7201780415430267\n",
      "Recall: 0.7367941712204007\n",
      "F1 Score: 0.728391356542617\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.0000\t\u001a?maybe        \t\t0.0533\tgreat          \n",
      "\t0.0000\t\u001a?maybe_i      \t\t0.0388\tnot            \n",
      "\t0.0000\t!!!!!--over    \t\t0.0347\texcellent      \n",
      "\t0.0000\t!!!!!--over_rated\t\t0.0252\twaste          \n",
      "\t0.0000\t!!!!finally    \t\t0.0208\tbest           \n",
      "\t0.0000\t!!!!finally_i  \t\t0.0177\tlove           \n",
      "\t0.0000\t!ps            \t\t0.0163\teasy           \n",
      "\t0.0000\t!ps_use        \t\t0.0154\tbad            \n",
      "\t0.0000\t\"              \t\t0.0125\tworst          \n",
      "\t0.0000\t\"$9            \t\t0.0078\ta_must         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Decision Tree**\n",
    "\n",
    "Trains the dataset over a Decision Tree with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(DecisionTreeClassifier())>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(DecisionTreeClassifier())\n",
    "classifier.train(BinaryBoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7286171672511054\n",
      "Precision: 0.7287009063444109\n",
      "Recall: 0.73224043715847\n",
      "F1 Score: 0.7304663840096911\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.0000\t\u001a?maybe        \t\t0.0533\tgreat          \n",
      "\t0.0000\t\u001a?maybe_i      \t\t0.0353\texcellent      \n",
      "\t0.0000\t!!!!!--over    \t\t0.0339\tnot            \n",
      "\t0.0000\t!!!!!--over_rated\t\t0.0247\twaste          \n",
      "\t0.0000\t!!!!finally    \t\t0.0208\tbest           \n",
      "\t0.0000\t!!!!finally_i  \t\t0.0177\tlove           \n",
      "\t0.0000\t!ps            \t\t0.0173\tbad            \n",
      "\t0.0000\t!ps_use        \t\t0.0167\teasy           \n",
      "\t0.0000\t\"              \t\t0.0125\tworst          \n",
      "\t0.0000\t\"$9            \t\t0.0078\ta_must         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Decision Tree**\n",
    "\n",
    "Trains the dataset over a Decision Tree with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(DecisionTreeClassifier())>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(DecisionTreeClassifier())\n",
    "classifier.train(LexiconFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.98875\n",
      "Precision: 0.9972024415055951\n",
      "Recall: 0.98025\n",
      "F1 Score: 0.9886535552193646\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.6583320628144534\n",
      "Precision: 0.6625501698054955\n",
      "Recall: 0.6514875531268974\n",
      "F1 Score: 0.6569722945048218\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.0378\tcount_neg      \t\t0.2876\tporc_neg       \n",
      "\t0.0634\tcount_pos      \t\t0.2520\tporc_pos       \n",
      "\t0.0865\tpoint_neg      \t\t0.1364\tpoint_pos      \n",
      "\t0.1363\tsize           \t\t0.1363\tsize           \n",
      "\t0.1364\tpoint_pos      \t\t0.0865\tpoint_neg      \n",
      "\t0.2520\tporc_pos       \t\t0.0634\tcount_pos      \n",
      "\t0.2876\tporc_neg       \t\t0.0378\tcount_neg      \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Results - Unified Data**\n",
    "\n",
    "| Combination | Accuracy | Precision | Recall | F1 Score|\n",
    "| --- | ----------- |----------- |----------- |----------- |\n",
    "| **BOW - DT** | 0.7240432992834274 | 0.7201780415430267 | **0.7367941712204007** | 0.728391356542617 |\n",
    "| **BBOW - DT** | **0.7286171672511054** | **0.7287009063444109** | 0.73224043715847 | **0.7304663840096911** |\n",
    "| **Lexicon - DT** | 0.6583320628144534 | 0.6625501698054955 | 0.6514875531268974 | 0.6569722945048218 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
