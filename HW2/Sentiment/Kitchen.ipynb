{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "**I. Individual Analysis**\n",
    "\n",
    "**Kitchen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics.scores import (precision, recall, accuracy)\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features Construction**\n",
    "\n",
    "Useful functions to load the different features models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = int(theword[1])\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_bow(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags in the filename and appends them to the\n",
    "    features list. Filters them to change it to 1 for each \n",
    "    existent element in the Bag of Words.\n",
    "    '''\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats = {}\n",
    "        for word in range(len(splited)-1):\n",
    "            theword = splited[word].split(\":\")\n",
    "            feats[theword[0]] = 1\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats,tag))\n",
    "        line = f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(thefeatures,filename):\n",
    "    '''\n",
    "    Loads the tags process them to create the Features based on the\n",
    "    AFINN-111 sentiment lexicon.\n",
    "    '''\n",
    "    lex = load_affin()\n",
    "    f = open(filename, \"r\")\n",
    "    line = f.readline()\n",
    "    while len(line) >0:\n",
    "        splited = line.split(' ')\n",
    "        feats1 = {}\n",
    "        summa = 0\n",
    "        negga = 0\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        tot = 0\n",
    "        \n",
    "        for word in range(len(splited)-1):\n",
    "            add = False\n",
    "            theword = splited[word].split(\":\")\n",
    "            lilwords = theword[0].split(\"_\")\n",
    "            for lw in lilwords:\n",
    "                if lw in lex:\n",
    "                    add = True\n",
    "                    if lex[lw] > 0:\n",
    "                        summa += lex[lw]*int(theword[1])\n",
    "                        pos += int(theword[1])\n",
    "                    else:\n",
    "                        negga += lex[lw]*int(theword[1])\n",
    "                        neg += int(theword[1])\n",
    "            tot += int(theword[1])\n",
    "        feats1[\"point_pos\"] = summa\n",
    "        feats1[\"point_neg\"] = -negga\n",
    "        feats1[\"count_pos\"] = pos\n",
    "        feats1[\"count_neg\"] = neg\n",
    "        feats1[\"size\"] = tot\n",
    "        feats1[\"porc_pos\"] = pos/tot\n",
    "        feats1[\"porc_neg\"] = neg/tot\n",
    "        tag = splited[-1].split(\"#:\")[1][0:3]\n",
    "        thefeatures.append((feats1,tag))\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "def load_affin():\n",
    "    '''\n",
    "    Loads the lexicon of sentiment words included in the AFINN-11 lexicon\n",
    "    '''\n",
    "    f = open(\"AFINN-111.txt\", \"r\")\n",
    "    line = f.readline()\n",
    "    affin = {}\n",
    "    while len(line) >0:\n",
    "        splited = line.split('\\t')\n",
    "        affin[splited[0]] = int(splited[1])\n",
    "        line = f.readline()\n",
    "    return affin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Function*\n",
    "\n",
    "Shows the most informative features for the Linear Regression Models.\n",
    "\n",
    "Taken from the answers to: https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers Kudos to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''\n",
    "    Gets the most informative features from a Linear Regression Model\n",
    "    '''\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_bow(BoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_bow(BoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5945 reviews\n"
     ]
    }
   ],
   "source": [
    "BoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_bow(BoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Filters them to change it to 1 for each existent element in the Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_binary_bow(BinaryBoWFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5945 reviews\n"
     ]
    }
   ],
   "source": [
    "BinaryBoWTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_binary_bow(BinaryBoWTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(BinaryBoWTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features**\n",
    "\n",
    "Loads the data and creates the feature to train. Loads and uses both review files. Takes into use the following Features:\n",
    "\n",
    "- Number of positive words\n",
    "- Number of negative words\n",
    "- Positive score following the AFINN-111 scores\n",
    "- Negative score following the AFINN-111 scores\n",
    "- Size (number of words) of the review\n",
    "- Percentage of positive words\n",
    "- Percentage of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconFeatures = []\n",
    "\n",
    "#Loads the positive reviews\n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/positive.review\")\n",
    "\n",
    "#Loads the negative reviews \n",
    "load_lexicon(LexiconFeatures,\"reviews/kitchen/negative.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconFeatures),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the data and creates the feature to test the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5945 reviews\n"
     ]
    }
   ],
   "source": [
    "LexiconTesting =[]\n",
    "\n",
    "#Loads the testing reviews \n",
    "load_lexicon(LexiconTesting,\"reviews/kitchen/unlabeled.review\")\n",
    "\n",
    "print(\"Loaded\",len(LexiconTesting),\"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**\n",
    "\n",
    "Now we are gonna test the six different combinations of Features and classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              your_money = 1                 neg : pos    =     35.7 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     30.3 : 1.0\n",
      "                    easy = 2                 pos : neg    =     26.3 : 1.0\n",
      "       very_disappointed = 1                 neg : pos    =     22.3 : 1.0\n",
      "                 awesome = 1                 pos : neg    =     21.0 : 1.0\n",
      "                   waste = 1                 neg : pos    =     19.3 : 1.0\n",
      "                 not_buy = 1                 neg : pos    =     19.0 : 1.0\n",
      "                 easy_to = 2                 pos : neg    =     18.3 : 1.0\n",
      "               love_this = 1                 pos : neg    =     17.3 : 1.0\n",
      "                returned = 1                 neg : pos    =     15.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.9985\n",
      "Precision: 0.998001998001998\n",
      "Recall: 0.999\n",
      "F1 Score: 0.9985007496251875\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8785534062237174\n",
      "Precision: 0.8932346723044398\n",
      "Recall: 0.8581584292484766\n",
      "F1 Score: 0.8753453038674034\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8772077375946173\n",
      "Precision: 0.8779741672331747\n",
      "Recall: 0.8744075829383886\n",
      "F1 Score: 0.8761872455902306\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.1427\tdisappointed   \t\t1.4160\tgreat          \n",
      "\t-1.0509\tnot            \t\t1.1200\teasy           \n",
      "\t-0.8586\tpoor           \t\t1.0313\tbest           \n",
      "\t-0.7247\ttoo            \t\t1.0042\tlove           \n",
      "\t-0.7042\treturn         \t\t1.0012\texcellent      \n",
      "\t-0.5943\tbroken         \t\t0.9380\tperfect        \n",
      "\t-0.5803\tbroke          \t\t0.8384\tworks          \n",
      "\t-0.5785\tdisappointment \t\t0.7886\teasy_to        \n",
      "\t-0.5689\tdoesn't        \t\t0.6745\twell           \n",
      "\t-0.5678\twaste          \t\t0.6228\tgood           \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              your_money = 1                 neg : pos    =     37.0 : 1.0\n",
      "              waste_your = 1                 neg : pos    =     30.3 : 1.0\n",
      "       very_disappointed = 1                 neg : pos    =     23.0 : 1.0\n",
      "                 awesome = 1                 pos : neg    =     21.0 : 1.0\n",
      "                 not_buy = 1                 neg : pos    =     20.3 : 1.0\n",
      "               defective = 1                 neg : pos    =     18.3 : 1.0\n",
      "               love_this = 1                 pos : neg    =     18.1 : 1.0\n",
      "                   loves = 1                 pos : neg    =     17.7 : 1.0\n",
      "                returned = 1                 neg : pos    =     17.3 : 1.0\n",
      "                  refund = 1                 neg : pos    =     17.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(BinaryBoWFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.999\n",
      "Precision: 0.999\n",
      "Recall: 0.999\n",
      "F1 Score: 0.9989999999999999\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.880067283431455\n",
      "Precision: 0.8932958932958933\n",
      "Recall: 0.8615436696005416\n",
      "F1 Score: 0.8771325176632775\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary BoW - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Binary Bag of Words Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(BinaryBoWFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(BinaryBoWFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.8809083263246426\n",
      "Precision: 0.88406292749658\n",
      "Recall: 0.8750846310088016\n",
      "F1 Score: 0.8795508676420551\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(BinaryBoWTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.3309\tnot            \t\t1.5681\tgreat          \n",
      "\t-1.2267\tdisappointed   \t\t1.1500\teasy           \n",
      "\t-0.8215\tpoor           \t\t1.0546\tlove           \n",
      "\t-0.7691\ttoo            \t\t1.0493\tbest           \n",
      "\t-0.6934\tbroke          \t\t1.0441\texcellent      \n",
      "\t-0.6917\tafter          \t\t0.9789\tperfect        \n",
      "\t-0.6860\treturn         \t\t0.8418\tworks          \n",
      "\t-0.6291\treturned       \t\t0.8200\teasy_to        \n",
      "\t-0.6082\twaste          \t\t0.7031\tgood           \n",
      "\t-0.6058\tdisappointment \t\t0.6077\twell           \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Naive Bayes**\n",
    "\n",
    "Trains the dataset over a Naive Bayes with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               count_pos = 16                pos : neg    =     13.0 : 1.0\n",
      "               count_neg = 14                neg : pos    =     11.8 : 1.0\n",
      "               count_pos = 10                pos : neg    =      9.0 : 1.0\n",
      "               count_neg = 11                neg : pos    =      8.6 : 1.0\n",
      "               point_neg = 5                 neg : pos    =      8.3 : 1.0\n",
      "               count_pos = 0                 neg : pos    =      7.9 : 1.0\n",
      "                porc_pos = 0.0               neg : pos    =      7.9 : 1.0\n",
      "               point_pos = 0                 neg : pos    =      7.9 : 1.0\n",
      "               count_neg = 4                 neg : pos    =      7.7 : 1.0\n",
      "                    size = 33                pos : neg    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.NaiveBayesClassifier.train(LexiconFeatures)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.89\n",
      "Precision: 0.8793774319066148\n",
      "Recall: 0.904\n",
      "F1 Score: 0.8915187376725837\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7312026913372582\n",
      "Precision: 0.720703125\n",
      "Recall: 0.7494922139471902\n",
      "F1 Score: 0.7348157982077663\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon Features - Linear Regression**\n",
    "\n",
    "Trains the dataset over a Linear Regression with a Lexicon Features Model.\n",
    "\n",
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LogisticRegression(max_iter=1000))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(LexiconFeatures)\n",
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "classifier.train(LexiconFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Validating set:\n",
      "Accuracy: 0.771\n",
      "Precision: 0.7811203319502075\n",
      "Recall: 0.753\n",
      "F1 Score: 0.7668024439918534\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconFeatures):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For Validating set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set:\n",
      "Accuracy: 0.7613120269133726\n",
      "Precision: 0.7530497856907352\n",
      "Recall: 0.7731888964116452\n",
      "F1 Score: 0.7629864706864874\n"
     ]
    }
   ],
   "source": [
    "refsets = nltk.collections.defaultdict(set)\n",
    "testsets = nltk.collections.defaultdict(set)\n",
    "real = []\n",
    "result = []\n",
    "\n",
    "for i, (feats, label) in enumerate(LexiconTesting):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    real.append(label)\n",
    "    result.append(observed)\n",
    "\n",
    "print( 'For testing set:')\n",
    "print( 'Accuracy:', nltk.accuracy(real,result) )\n",
    "print( 'Precision:', nltk.scores.precision(refsets['pos'], testsets['pos']) )\n",
    "print( 'Recall:', nltk.recall(refsets['pos'], testsets['pos']) )\n",
    "print( 'F1 Score:', nltk.f_measure(refsets['pos'], testsets['pos']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-1.4461\tporc_neg       \t\t3.1889\tporc_pos       \n",
      "\t-0.0875\tpoint_neg      \t\t0.0735\tpoint_pos      \n",
      "\t-0.0168\tcount_pos      \t\t0.0003\tcount_neg      \n",
      "\t-0.0019\tsize           \t\t-0.0019\tsize           \n",
      "\t0.0003\tcount_neg      \t\t-0.0168\tcount_pos      \n",
      "\t0.0735\tpoint_pos      \t\t-0.0875\tpoint_neg      \n",
      "\t3.1889\tporc_pos       \t\t-1.4461\tporc_neg       \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(classifier._vectorizer,classifier._clf,10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Results - Kitchen**\n",
    "\n",
    "| Combination | Accuracy | Precision | Recall | F1 Score|\n",
    "| --- | ----------- |----------- |----------- |----------- |\n",
    "| **BOW - NB** | 0.8785534062237174 | 0.8932346723044398 | 0.8581584292484766 | 0.8753453038674034 |\n",
    "| **BOW - LR** | 0.8772077375946173 | 0.8779741672331747 | 0.8744075829383886 | 0.8761872455902306 |\n",
    "| **BBOW - NB** | 0.880067283431455 | **0.8932958932958933** | 0.8615436696005416 | 0.8771325176632775 |\n",
    "| **BBOW - LR** | **0.8809083263246426** | 0.88406292749658 | **0.8750846310088016** | **0.8795508676420551** |\n",
    "| **Lexicon - NB** | 0.7312026913372582 | 0.720703125 | 0.7494922139471902 | 0.7348157982077663 |\n",
    "| **Lexicon - LR** | 0.7613120269133726 | 0.7530497856907352 | 0.7731888964116452 | 0.7629864706864874 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
